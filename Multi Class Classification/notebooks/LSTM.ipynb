{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers text-hammer pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup , AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "import text_hammer as th\n",
    "import pandas as pd\n",
    "import tensorflow as tf , keras\n",
    "import transformers\n",
    "import random as rd\n",
    "import keras.backend as K\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79370020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51339654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(model, validation_dataloader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    return average_val_loss\n",
    "\n",
    "def metric2(y_true, y_pred_classes):\n",
    "    n = y_true.size(0)\n",
    "    # Calculate errors where prediction is off by 1 class\n",
    "    res = torch.abs(y_true - y_pred_classes)\n",
    "    count_error = torch.sum(res == 1, dtype=torch.float32)\n",
    "    metric = 1 - count_error / n\n",
    "    return metric.item()\n",
    "\n",
    "def metric2_2(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Calculate the number of predictions off by 1 class\n",
    "    off_by_one = np.sum(np.abs(y_true - y_pred) == 1)\n",
    "    # Calculate the metric\n",
    "    metric = 1 - off_by_one / len(y_true)\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    mention = r'@\\w+'\n",
    "    hash = r'#\\w+'\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', '')\n",
    "    x = re.sub(r'[^\\x00-\\x7F]+', ' ', x)\n",
    "    x = th.cont_exp(x)\n",
    "    x = th.remove_emails(x)\n",
    "    x = th.remove_urls(x)\n",
    "    x = re.sub(mention, ' ', x)\n",
    "    x = re.sub(hash, ' ', x)\n",
    "    x = th.remove_html_tags(x)\n",
    "    x = th.remove_rt(x)\n",
    "    x = th.remove_accented_chars(x)\n",
    "    x = th.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    x = re.sub(r'\\w*\\d+\\w*', ' ', x).strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d79553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_spss(\"/content/drive/MyDrive/VA_EN_TU_2012-2020_3000_tweets_relevant_V03_labeled_1200_cleaned.sav\")\n",
    "# Original labels: 0 - positive, 1 - negative, 2 - neutral\n",
    "# Remapping dictionary to align with RoBERTa's expected labels\n",
    "label_mapping = {1: 2, 2: 0, 3: 1}\n",
    "\n",
    "# Remapped labels: 0 - negative, 1 - neutral, 2 - positive\n",
    "df['Label_B_emotion'] = df['Label_B_emotion'].replace(label_mapping)\n",
    "df['Label_B_emotion'] = df['Label_B_emotion'].astype(int)\n",
    "dff = df[['text','Label_B_emotion']].copy()\n",
    "dff['Label_B_emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['cleaned_data'] = dff['text'].apply(get_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dff['text'].tolist()\n",
    "y = dff['Label_B_emotion'].tolist()\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, maxlen=70)\n",
    "\n",
    "y_categorical = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_no = 1\n",
    "for train_index, val_index in kfold.split(X_train):\n",
    "\n",
    "    X_kf_train, X_kf_val = X_train[train_index], X_train[val_index]\n",
    "    y_kf_train, y_kf_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=128, input_length=70))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    model.fit(X_kf_train, y_kf_train, batch_size=64, epochs=5, validation_data=(X_kf_val, y_kf_val), verbose=1)\n",
    "\n",
    "    scores = model.evaluate(X_kf_val, y_kf_val, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    model.save(f'/content/LSTM/model_fold_{fold_no}.h5')\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1992de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(num_folds, X_test):\n",
    "    predictions = []\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        model = load_model(f'/content/LSTM/model_fold_{fold}.h5')\n",
    "        pred = model.predict(X_test)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    avg_pred = np.mean(predictions, axis=0)\n",
    "    return np.argmax(avg_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred_classes = ensemble_predictions(5, X_test)\n",
    "\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "ensemble_accuracy = accuracy_score(y_test_true_classes, ensemble_pred_classes)\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy}\")\n",
    "\n",
    "ensemble_metric2_score = metric2_2(y_test_true_classes, ensemble_pred_classes)\n",
    "print(f'Ensemble Metric2: {ensemble_metric2_score}')\n",
    "\n",
    "ensemble_conf_matrix = confusion_matrix(y_test_true_classes, ensemble_pred_classes)\n",
    "print(f'Ensemble Confusion Matrix:\\n{ensemble_conf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84143459",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(y_test_true_classes, ensemble_pred_classes,target_names=['Negative','Neutral','Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788209a6",
   "metadata": {},
   "source": [
    "#### Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dff['cleaned_data'].tolist()\n",
    "y = dff['Label_B_emotion'].tolist()\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, maxlen=70)\n",
    "\n",
    "y_categorical = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dff559",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_no = 1\n",
    "for train_index, val_index in kfold.split(X_train):\n",
    "\n",
    "    X_kf_train, X_kf_val = X_train[train_index], X_train[val_index]\n",
    "    y_kf_train, y_kf_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=128, input_length=70))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    model.fit(X_kf_train, y_kf_train, batch_size=64, epochs=5, validation_data=(X_kf_val, y_kf_val), verbose=1)\n",
    "\n",
    "    scores = model.evaluate(X_kf_val, y_kf_val, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    model.save(f'/content/LSTM_clean/model_fold_{fold_no}.h5')\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(num_folds, X_test):\n",
    "    predictions = []\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        model = load_model(f'/content/LSTM_clean/model_fold_{fold}.h5')\n",
    "        pred = model.predict(X_test)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    avg_pred = np.mean(predictions, axis=0)\n",
    "    return np.argmax(avg_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1187d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred_classes = ensemble_predictions(5, X_test)\n",
    "\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "ensemble_accuracy = accuracy_score(y_test_true_classes, ensemble_pred_classes)\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy}\")\n",
    "\n",
    "ensemble_metric2_score = metric2_2(y_test_true_classes, ensemble_pred_classes)\n",
    "print(f'Ensemble Metric2: {ensemble_metric2_score}')\n",
    "\n",
    "ensemble_conf_matrix = confusion_matrix(y_test_true_classes, ensemble_pred_classes)\n",
    "print(f'Ensemble Confusion Matrix:\\n{ensemble_conf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfdf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(y_test_true_classes, ensemble_pred_classes,target_names=['Negative','Neutral','Positive']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
