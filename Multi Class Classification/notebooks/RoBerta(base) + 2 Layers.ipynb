{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers text-hammer pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup , AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "import text_hammer as th\n",
    "import pandas as pd\n",
    "import tensorflow as tf , keras\n",
    "import transformers\n",
    "import random as rd\n",
    "import keras.backend as K\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79370020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51339654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(model, validation_dataloader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    return average_val_loss\n",
    "\n",
    "def metric2(y_true, y_pred_classes):\n",
    "    n = y_true.size(0)\n",
    "    # Calculate errors where prediction is off by 1 class\n",
    "    res = torch.abs(y_true - y_pred_classes)\n",
    "    count_error = torch.sum(res == 1, dtype=torch.float32)\n",
    "    metric = 1 - count_error / n\n",
    "    return metric.item()\n",
    "\n",
    "def metric2_2(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Calculate the number of predictions off by 1 class\n",
    "    off_by_one = np.sum(np.abs(y_true - y_pred) == 1)\n",
    "    # Calculate the metric\n",
    "    metric = 1 - off_by_one / len(y_true)\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    mention = r'@\\w+'\n",
    "    hash = r'#\\w+'\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', '')\n",
    "    x = re.sub(r'[^\\x00-\\x7F]+', ' ', x)\n",
    "    x = th.cont_exp(x)\n",
    "    x = th.remove_emails(x)\n",
    "    x = th.remove_urls(x)\n",
    "    x = re.sub(mention, ' ', x)\n",
    "    x = re.sub(hash, ' ', x)\n",
    "    x = th.remove_html_tags(x)\n",
    "    x = th.remove_rt(x)\n",
    "    x = th.remove_accented_chars(x)\n",
    "    x = th.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    x = re.sub(r'\\w*\\d+\\w*', ' ', x).strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d79553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_spss(\"/content/drive/MyDrive/VA_EN_TU_2012-2020_3000_tweets_relevant_V03_labeled_1200_cleaned.sav\")\n",
    "# Original labels: 0 - positive, 1 - negative, 2 - neutral\n",
    "# Remapping dictionary to align with RoBERTa's expected labels\n",
    "label_mapping = {1: 2, 2: 0, 3: 1}\n",
    "\n",
    "# Remapped labels: 0 - negative, 1 - neutral, 2 - positive\n",
    "df['Label_B_emotion'] = df['Label_B_emotion'].replace(label_mapping)\n",
    "df['Label_B_emotion'] = df['Label_B_emotion'].astype(int)\n",
    "dff = df[['text','Label_B_emotion']].copy()\n",
    "dff['Label_B_emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['cleaned_data'] = dff['text'].apply(get_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = len(dff)\n",
    "proportion_training = 0.80\n",
    "pretrained_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "seq_len = 128\n",
    "batch_size = 40\n",
    "learning_rate = 2e-5\n",
    "nb_epoch = 60\n",
    "nb_fold = 3\n",
    "earlyStopPatience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, max_length=seq_len,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ids = np.zeros((len(dff),seq_len))\n",
    "X_mask = np.zeros((len(dff),seq_len))\n",
    "\n",
    "for i, sentence in enumerate(dff['text']):\n",
    "    X_ids[i, :], X_mask[i, :] = tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = round(proportion_training * len(dff))\n",
    "labels = dff[['Label_B_emotion']].values\n",
    "\n",
    "X_ids_train = X_ids[:sep,:]\n",
    "X_mask_train = X_mask[:sep,:]\n",
    "Y_train = labels[:sep,:]\n",
    "\n",
    "X_ids_test = X_ids[sep:,:]\n",
    "X_mask_test = X_mask[sep:,:]\n",
    "Y_test = labels[sep:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric1(y_true, y_pred):\n",
    "    n = K.cast(K.shape(y_true)[0], 'float32')\n",
    "    y_pred = K.reshape(y_pred,(n,1))\n",
    "    res = K.equal(y_true,y_pred)\n",
    "    res = K.cast(res, 'float32')\n",
    "    res = K.sum(res) / n\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64916860",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = TFAutoModel.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    input_text_ids = keras.Input(shape=(seq_len,), dtype='int32', name='input_text_ids')\n",
    "    text_mask = keras.Input(shape=(seq_len,), dtype='int32', name='attention_mask_text')\n",
    "\n",
    "    embeddings = pretrained_model(input_text_ids, attention_mask=text_mask)[1]\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    predictions = layers.Dense(3, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[input_text_ids,text_mask], outputs=predictions)\n",
    "\n",
    "    model.layers[2].trainable = False\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=\"accuracy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1011995",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422525c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_the_model(model, X_tr, y_tr, X_val, y_val, num):\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_accuracy\", patience=earlyStopPatience, mode=\"max\"),\n",
    "        ModelCheckpoint(filepath=\"MODEL/best_model\"+str(num)+\".hdf5\", monitor=\"val_accuracy\", mode='max', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    model.fit(X_tr, y_tr, validation_data = (X_val,y_val), epochs=nb_epoch, verbose=1, callbacks=callbacks, batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e155bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = []\n",
    "list_history = []\n",
    "\n",
    "kfold = KFold(n_splits=nb_fold, shuffle=True, random_state=42)\n",
    "\n",
    "# Training with k-fold method\n",
    "k = 1\n",
    "for train_index, val_index in kfold.split(X_ids_train,Y_train):\n",
    "    print(f'\\nTraining model {k}...')\n",
    "    model = get_model()\n",
    "    history = fit_the_model(model,\n",
    "                            [X_ids_train[train_index],\n",
    "                             X_mask_train[train_index]],\n",
    "                            Y_train[train_index],\n",
    "                            [X_ids_train[val_index],\n",
    "                             X_mask_train[val_index]],\n",
    "                            Y_train[val_index], k)\n",
    "    list_of_models.append(history)\n",
    "    list_history.append(history.history.history)\n",
    "    k += 1\n",
    "\n",
    "print(\"---Finished---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_true , y_pred):\n",
    "    cm = confusion_matrix(y_true , y_pred)\n",
    "    return cm/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2_per_class(y_true,y_pred):\n",
    "    L = []\n",
    "    y_pred = y_pred.reshape((len(y_pred),1))\n",
    "    for i in range(0,3):\n",
    "        L.append(y_true[(y_true==y_pred) & (y_true==i)].shape[0] / y_true[(y_true==i)].shape[0])\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53266ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_fold):\n",
    "    print(50*'-',f\"MODEL PERFORMANCE {i+1}\",50*'-')\n",
    "\n",
    "    average_loss = list_history[i]['loss']\n",
    "    average_val_loss = list_history[i]['val_loss']\n",
    "\n",
    "    average_metric_1 = list_history[i]['accuracy']\n",
    "    average_val_metric_1 = list_history[i]['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(16,4))\n",
    "\n",
    "    txtTitres = [\"Training and validation loss\",\"Training and validation accuracy\"]\n",
    "    txtYLabel = [\"Loss\",\"Metric 1 \"]\n",
    "\n",
    "    L1 = [average_loss, average_metric_1]\n",
    "    L2 = [average_val_loss, average_val_metric_1]\n",
    "\n",
    "    for k in range(2):\n",
    "        if len(L1[0]) < nb_epoch:\n",
    "            nb_epoch_plot=len(L1[0])\n",
    "        else:\n",
    "            nb_epoch_plot=nb_epoch\n",
    "        Epochs = range(1, nb_epoch_plot+1)\n",
    "        plt.subplot(1,3,k+1)\n",
    "        plt.plot(Epochs, L1[k], color='b', marker='o', label=\"training\")\n",
    "        plt.plot(Epochs, L2[k], color='r', marker='o', label=\"validation\")\n",
    "        plt.grid()\n",
    "        plt.title(txtTitres[k])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(txtYLabel[k])\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_best_models = []\n",
    "for i in range(nb_fold):\n",
    "    path = \"MODEL/best_model\" + str(i+1) + \".hdf5\"\n",
    "    L_best_models.append(keras.models.load_model(path,\n",
    "                                                 custom_objects={\"TFRobertaModel\": pretrained_model},\n",
    "                                                 compile=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30eeb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_predictions = []\n",
    "L_cm = []\n",
    "L_f1 = []\n",
    "L_m1, L_m2 = [], []\n",
    "\n",
    "k = 0\n",
    "\n",
    "kfold = KFold(n_splits=nb_fold, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kfold.split(X_ids_train,Y_train):\n",
    "\n",
    "    prediction = np.argmax(L_best_models[k].predict([X_ids_train[val_index], X_mask_train[val_index]]), axis=1)\n",
    "\n",
    "    cm = ConfusionMatrix(Y_train[val_index] , prediction)\n",
    "    f1 = f1_score(Y_train[val_index] , prediction, average=None)\n",
    "    m1 = metric1(Y_train[val_index], prediction)\n",
    "    m2 = metric2_per_class(Y_train[val_index] , prediction)\n",
    "\n",
    "    L_predictions.append(prediction)\n",
    "    L_cm.append(cm)\n",
    "    L_f1.append(f1)\n",
    "    L_m1.append(m1)\n",
    "    L_m2.append(m2)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(40*'-',f\"SUMMARY OF MODEL PERFORMANCE ON VALIDATION DATA\",20*'-',\"\\n\")\n",
    "\n",
    "m1 = np.mean(L_m1,axis=0)\n",
    "m2 = np.mean(L_m2,axis=0)\n",
    "\n",
    "print(\"metric1 validation mean :\", m1, \"\\n\")\n",
    "print(\"metric2 validation mean per class :\", m2, \"\\n\")\n",
    "\n",
    "print(\"Average F1 score on positive/negative/neutral: \", np.mean(L_f1,axis=0))\n",
    "\n",
    "avg_cm = np.mean(L_cm,axis=0)\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = sns.heatmap(avg_cm, annot=True, fmt='.2%' , cmap=\"Blues\")\n",
    "ax.set_xlabel('\\n Predicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "ax.set_title(f'Confusion Matrix of the model ')\n",
    "ax.xaxis.set_ticklabels(['Negative', 'Neutral' ,'Positive'])\n",
    "ax.yaxis.set_ticklabels(['Negative', 'Neutral' ,'Positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_pred = [np.round(model.predict([X_ids_test, X_mask_test])) for model in L_best_models]\n",
    "prediction = np.median(L_pred, axis=0)\n",
    "\n",
    "predicted_labels = np.argmax(prediction, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ead08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(Y_test, predicted_labels,target_names=['Negative','Neutral','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(Y_test , predicted_labels)\n",
    "f1 = f1_score(Y_test , predicted_labels, average=None)\n",
    "m1 = float(metric1(Y_test, predicted_labels))\n",
    "m2 = float(metric2_2(Y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c68ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"metric1 Test mean :\", m1, \"\\n\")\n",
    "print(\"metric2 Test mean :\", m2, \"\\n\")\n",
    "print(\"Average F1 score on negative/neutral/positive: \", np.mean(f1,axis=0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = sns.heatmap(cm, annot=True, fmt='.2%' , cmap=\"Blues\")\n",
    "ax.set_xlabel('\\n Predicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "ax.set_title(f'Confusion Matrix of the model ')\n",
    "ax.xaxis.set_ticklabels(['Negative', 'Neutral' ,'Positive'])\n",
    "ax.yaxis.set_ticklabels(['Negative', 'Neutral' ,'Positive'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
