{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ad8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers text-hammer pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd54caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup , AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "import text_hammer as th\n",
    "import pandas as pd\n",
    "import tensorflow as tf , keras\n",
    "import transformers\n",
    "import random as rd\n",
    "import keras.backend as K\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9746d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(model, validation_dataloader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    return average_val_loss\n",
    "\n",
    "def metric2(y_true, y_pred_classes):\n",
    "    n = y_true.size(0)\n",
    "    # Calculate errors where prediction is off by 1 class\n",
    "    res = torch.abs(y_true - y_pred_classes)\n",
    "    count_error = torch.sum(res == 1, dtype=torch.float32)\n",
    "    metric = 1 - count_error / n\n",
    "    return metric.item()\n",
    "\n",
    "def metric2_2(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Calculate the number of predictions off by 1 class\n",
    "    off_by_one = np.sum(np.abs(y_true - y_pred) == 1)\n",
    "    # Calculate the metric\n",
    "    metric = 1 - off_by_one / len(y_true)\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07920cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    mention = r'@\\w+'\n",
    "    hash = r'#\\w+'\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', '')\n",
    "    x = re.sub(r'[^\\x00-\\x7F]+', ' ', x)\n",
    "    x = th.cont_exp(x)\n",
    "    x = th.remove_emails(x)\n",
    "    x = th.remove_urls(x)\n",
    "    x = re.sub(mention, ' ', x)\n",
    "    x = re.sub(hash, ' ', x)\n",
    "    x = th.remove_html_tags(x)\n",
    "    x = th.remove_rt(x)\n",
    "    x = th.remove_accented_chars(x)\n",
    "    x = th.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    x = re.sub(r'\\w*\\d+\\w*', ' ', x).strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_spss(\"/content/drive/MyDrive/VA_EN_TU_2012-2020_3000_tweets_relevant_V03_labeled_1200_cleaned.sav\")\n",
    "# Original labels: 0 - positive, 1 - negative, 2 - neutral\n",
    "# Remapping dictionary to align with RoBERTa's expected labels\n",
    "label_mapping = {1: 2, 2: 0, 3: 1}\n",
    "\n",
    "# Remapped labels: 0 - negative, 1 - neutral, 2 - positive\n",
    "df['Label_B_emotion'] = df['Label_B_emotion'].replace(label_mapping)\n",
    "df['Label_B_emotion'] = df['Label_B_emotion'].astype(int)\n",
    "dff = df[['text','Label_B_emotion']].copy()\n",
    "dff['Label_B_emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['cleaned_data'] = dff['text'].apply(get_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_temp, labels_train, labels_temp = train_test_split(\n",
    "    dff[\"text\"],\n",
    "    dff[\"Label_B_emotion\"],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the temporary test set into validation and test sets\n",
    "data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "    data_temp,\n",
    "    labels_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tokens = ['self-driving', 'autopilot', 'driverless', 'lidar', 'driver-less','automated','mobility','autonomous',\n",
    "#                'traffic-law','safety-standards','smart-car','hands-free','AI-powered','selfdriving','IoT'\n",
    "#                ]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "# num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# print('Added for : twitter-roberta-base-sentiment ', num_added_toks, 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode training data\n",
    "encoded_train = tokenizer(\n",
    "    text=data_train.tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    max_length=128\n",
    ")\n",
    "train_input_ids = encoded_train[\"input_ids\"].to(device)\n",
    "train_attention_mask = encoded_train[\"attention_mask\"].to(device)\n",
    "train_labels = torch.tensor(labels_train.tolist()).to(device)\n",
    "\n",
    "# Tokenize and encode validation data\n",
    "encoded_val = tokenizer(\n",
    "    text=data_val.tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    max_length=128\n",
    ")\n",
    "val_input_ids = encoded_val[\"input_ids\"].to(device)\n",
    "val_attention_mask = encoded_val[\"attention_mask\"].to(device)\n",
    "val_labels = torch.tensor(labels_val.tolist()).to(device)\n",
    "\n",
    "# Tokenize and encode test data\n",
    "encoded_test = tokenizer(\n",
    "    text=data_test.tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    max_length=128\n",
    ")\n",
    "test_input_ids = encoded_test[\"input_ids\"].to(device)\n",
    "test_attention_mask = encoded_test[\"attention_mask\"].to(device)\n",
    "test_labels = torch.tensor(labels_test.tolist()).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd830af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader objects\n",
    "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=40, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=40)\n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802454e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization and training setup\n",
    "num_labels = 3\n",
    "model = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment', num_labels=num_labels)\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "for layer in model.roberta.encoder.layer:\n",
    "    layer.attention.self.dropout.p = 0.5\n",
    "    layer.attention.output.dropout.p = 0.5\n",
    "\n",
    "# Set dropout rate in classifier layer\n",
    "model.classifier.dropout.p = 0.2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 2)\n",
    "num_epochs = 2\n",
    "model.to(device)\n",
    "\n",
    "print(\"Attention dropout rate:\", model.roberta.encoder.layer[0].attention.self.dropout.p)\n",
    "print(\"Classifier dropout rate:\", model.classifier.dropout.p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca15acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_predictions, val_true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            val_predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "            val_true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        #torch.save(model.state_dict(), \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ba8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "        true_labels.extend(labels.cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b82144",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "report = classification_report(true_labels, predictions, target_names=target_names)\n",
    "metric2_value = metric2(torch.tensor(true_labels), torch.tensor(predictions))\n",
    "print(f\"Test Metric 2: {metric2_value:.4f}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28125afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names,\n",
    "            yticklabels=target_names)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix [Twitter-Roberta-base extended vocabulary]: raw data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29750f89",
   "metadata": {},
   "source": [
    "#### Version 2 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoConfig\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        #item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "num_labels = 3\n",
    "model = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment', num_labels=num_labels)\n",
    "\n",
    "for layer in model.roberta.encoder.layer:\n",
    "    layer.attention.self.dropout.p = 0.5\n",
    "    layer.attention.output.dropout.p = 0.5\n",
    "\n",
    "model.classifier.dropout.p = 0.2\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "model.to(device)\n",
    "\n",
    "train_df , test_df = train_test_split(data_f[['text','Label_B_emotion']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the input\n",
    "encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "labels = train_df['Label_B_emotion'].tolist()\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TweetDataset(encodings, labels)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60282820",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_labels = test_df['Label_B_emotion'].tolist()\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n",
    "\n",
    "evaluation_results = trainer.evaluate(test_dataset)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4796ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import torch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    preds_tensor = torch.tensor(preds)\n",
    "\n",
    "    # Metrics\n",
    "    metric_2 = metric2(labels_tensor, preds_tensor)\n",
    "    metric_1 = metric1_torch(labels_tensor, preds_tensor)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'metric1': metric_1,\n",
    "        'metric2': metric_2\n",
    "    }\n",
    "\n",
    "def metric2(y_true, y_pred_classes):\n",
    "    n = y_true.size(0)\n",
    "    # Calculate errors where prediction is off by 1 class\n",
    "    res = torch.abs(y_true - y_pred_classes)\n",
    "    count_error = torch.sum(res == 1, dtype=torch.float32)\n",
    "    metric = 1 - count_error / n\n",
    "    return metric.item()\n",
    "\n",
    "def metric1_torch(y_true, y_pred):\n",
    "    correct_preds = (y_true == y_pred)\n",
    "    metric = torch.mean(correct_preds.float())\n",
    "    return metric\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "evaluation_results = trainer.evaluate(test_dataset)\n",
    "print(evaluation_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
