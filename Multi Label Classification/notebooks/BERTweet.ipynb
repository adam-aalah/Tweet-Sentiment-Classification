{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers text-hammer pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f228ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup , AutoTokenizer, TFAutoModel, TFRobertaModel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "import text_hammer as th\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "import random as rd\n",
    "import keras.backend as K\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971a17f",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc605618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------STEP 1 --------------------------------#\n",
    "\n",
    "# Metric1 construction to compute the % of success tweet by tweet\n",
    "\n",
    "def metric1(y_true, y_pred):\n",
    "    n = K.cast(K.shape(y_true)[0], 'float32')\n",
    "    y_pred_r = K.round(y_pred)\n",
    "    res = K.all(K.equal(y_true, y_pred_r), axis=1)\n",
    "    res = K.cast(res, 'float32')\n",
    "    res = K.sum(res) / n\n",
    "    return res\n",
    "\n",
    "#------------------------STEP 2 --------------------------------#\n",
    "\n",
    "# Metric2 construction to compute the % of success label by label\n",
    "# (equal to binary-accuracy)\n",
    "\n",
    "def metric2(y_true, y_pred):\n",
    "    n = K.cast(K.shape(y_true)[0], 'float32')\n",
    "    y_pred_r = K.round(y_pred)\n",
    "    res = abs(y_true - y_pred_r)\n",
    "    count_error = K.sum(K.cast(K.equal(res, 1), 'float32'),axis=0)\n",
    "    res = 1-count_error / n\n",
    "    res = K.mean(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    mention = r'@\\w+'\n",
    "    hash = r'#\\w+'\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', '')\n",
    "    x = re.sub(r'[^\\x00-\\x7F]+', ' ', x)\n",
    "    x = th.cont_exp(x)\n",
    "    x = th.remove_emails(x)\n",
    "    x = th.remove_urls(x)\n",
    "    x = re.sub(mention, ' ', x)\n",
    "    x = re.sub(hash, ' ', x)\n",
    "    x = th.remove_html_tags(x)\n",
    "    x = th.remove_rt(x)\n",
    "    x = th.remove_accented_chars(x)\n",
    "    x = th.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    x = re.sub(r'\\w*\\d+\\w*', ' ', x).strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e4b31",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_spss(\"/content/drive/MyDrive/VA_EN_TU_2012-2020_3000_tweets_relevant_V03_labeled_1200_cleaned.sav\")\n",
    "data = df[['text', 'Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'] = data['text'].apply(get_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abaea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Individual Label Distribution\n",
    "label_sums = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].sum(axis=0)\n",
    "labels = ['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']\n",
    "\n",
    "# label_counts = data[['label_1_positive', 'label_2_negative', 'label_3_neutral']].sum()\n",
    "# print(label_counts)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(labels, label_sums)\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Individual Label Distribution')\n",
    "plt.show()\n",
    "\n",
    "## Label Combinations Distribution\n",
    "df_train_labels = pd.DataFrame(data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']], columns=labels)\n",
    "combinations = df_train_labels.groupby(labels).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "\n",
    "combinations = combinations.sort_values(by='count', ascending=False) # by count to get most frequent combinations\n",
    "combinations['Label Combination'] = combinations[labels].astype(int).astype(str).agg(','.join, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "combinations.plot(x='Label Combination', y='count', kind='bar', legend=False)\n",
    "plt.title('Label Combinations Distribution')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70780198",
   "metadata": {},
   "source": [
    "### BERTweet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load tokenizer and model\n",
    "pretrained_model_name = 'vinai/bertweet-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "pretrained_model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=3)\n",
    "\n",
    "# Define the model\n",
    "def get_model():\n",
    "\n",
    "    seq_len = 128  # Define your sequence length\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    output = pretrained_model(input_ids, attention_mask=attention_mask)[0]  # Using [0] to get the last hidden states\n",
    "\n",
    "    # Flatten the output or use global pooling\n",
    "    flat_output = tf.keras.layers.Flatten()(output)\n",
    "\n",
    "    # Add a dense layer for classification\n",
    "    predictions = tf.keras.layers.Dense(3, activation='sigmoid')(flat_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=predictions)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])  # You can change the metric as needed\n",
    "\n",
    "    return model\n",
    "\n",
    "# Model summary\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41daa656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_the_model(model, X_tr, y_tr, X_val, y_val, num):\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_accuracy\", patience=earlyStopPatience, mode=\"max\"),\n",
    "        ModelCheckpoint(filepath=\"MODEL_BERTweet/best_model\"+str(num)+\".hdf5\", monitor=\"val_accuracy\", mode='max', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    model.fit(X_tr, y_tr, validation_data = (X_val,y_val), epochs=nb_epoch, verbose=1, callbacks=callbacks, batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = []\n",
    "list_history = []\n",
    "\n",
    "kfold = KFold(n_splits=nb_fold, shuffle=True, random_state=42)\n",
    "\n",
    "# Training with k-fold method\n",
    "k = 1\n",
    "for train_index, val_index in kfold.split(X_ids_train,Y_train):\n",
    "    print(f'\\nTraining model {k}...')\n",
    "    model = get_model()\n",
    "    history = fit_the_model(model,\n",
    "                            [X_ids_train[train_index],\n",
    "                             X_mask_train[train_index]],\n",
    "                            Y_train[train_index],\n",
    "                            [X_ids_train[val_index],\n",
    "                             X_mask_train[val_index]],\n",
    "                            Y_train[val_index], k)\n",
    "    list_of_models.append(history)\n",
    "    list_history.append(history.history.history)\n",
    "    k += 1\n",
    "\n",
    "print(\"---Finished---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b39a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_true , y_pred):\n",
    "    cm = confusion_matrix(y_true , y_pred)\n",
    "    return cm/y_true.shape[0]\n",
    "\n",
    "def metric2_per_class(y_true,y_pred):\n",
    "    L = []\n",
    "    y_pred = y_pred.reshape((len(y_pred),1))\n",
    "    for i in range(0,3):\n",
    "        L.append(y_true[(y_true==y_pred) & (y_true==i)].shape[0] / y_true[(y_true==i)].shape[0])\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ac6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_fold):\n",
    "    print(50*'-',f\"MODEL PERFORMANCE {i+1}\",50*'-')\n",
    "\n",
    "    average_loss = list_history[i]['loss']\n",
    "    average_val_loss = list_history[i]['val_loss']\n",
    "\n",
    "    average_metric_1 = list_history[i]['accuracy']\n",
    "    average_val_metric_1 = list_history[i]['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(16,4))\n",
    "\n",
    "    txtTitres = [\"Training and validation loss\",\"Training and validation accuracy\"]\n",
    "    txtYLabel = [\"Loss\",\"Metric 1 \"]\n",
    "\n",
    "    L1 = [average_loss, average_metric_1]\n",
    "    L2 = [average_val_loss, average_val_metric_1]\n",
    "\n",
    "    for k in range(2):\n",
    "        if len(L1[0]) < nb_epoch:\n",
    "            nb_epoch_plot=len(L1[0])\n",
    "        else:\n",
    "            nb_epoch_plot=nb_epoch\n",
    "        Epochs = range(1, nb_epoch_plot+1)\n",
    "        plt.subplot(1,3,k+1)\n",
    "        plt.plot(Epochs, L1[k], color='b', marker='o', label=\"training\")\n",
    "        plt.plot(Epochs, L2[k], color='r', marker='o', label=\"validation\")\n",
    "        plt.grid()\n",
    "        plt.title(txtTitres[k])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(txtYLabel[k])\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_best_models = []\n",
    "for i in range(nb_fold):\n",
    "    path = \"MODEL_BERTweet/best_model\" + str(i+1) + \".hdf5\"\n",
    "    L_best_models.append(keras.models.load_model(path,\n",
    "                                                 custom_objects={\"TFRobertaModel\": pretrained_model},\n",
    "                                                 compile=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f99f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_predictions = []\n",
    "L_cm = []\n",
    "L_f1 = []\n",
    "L_m1, L_m2 = [], []\n",
    "\n",
    "k = 0\n",
    "\n",
    "kfold = KFold(n_splits=nb_fold, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kfold.split(X_ids_train,Y_train):\n",
    "\n",
    "    prediction = np.argmax(L_best_models[k].predict([X_ids_train[val_index], X_mask_train[val_index]]), axis=1)\n",
    "\n",
    "    cm = ConfusionMatrix(Y_train[val_index] , prediction)\n",
    "    f1 = f1_score(Y_train[val_index] , prediction, average=None)\n",
    "    m1 = metric1(Y_train[val_index], prediction)\n",
    "    m2 = metric2_per_class(Y_train[val_index] , prediction)\n",
    "\n",
    "    L_predictions.append(prediction)\n",
    "    L_cm.append(cm)\n",
    "    L_f1.append(f1)\n",
    "    L_m1.append(m1)\n",
    "    L_m2.append(m2)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(40*'-',f\"SUMMARY OF MODEL PERFORMANCE ON VALIDATION DATA\",20*'-',\"\\n\")\n",
    "\n",
    "m1 = np.mean(L_m1,axis=0)\n",
    "m2 = np.mean(L_m2,axis=0)\n",
    "\n",
    "print(\"metric1 validation mean :\", m1, \"\\n\")\n",
    "print(\"metric2 validation mean per class :\", m2, \"\\n\")\n",
    "\n",
    "print(\"Average F1 score on positive/negative/neutral: \", np.mean(L_f1,axis=0))\n",
    "\n",
    "avg_cm = np.mean(L_cm,axis=0)\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = sns.heatmap(avg_cm, annot=True, fmt='.2%' , cmap=\"Blues\")\n",
    "ax.set_xlabel('\\n Predicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "ax.set_title(f'Confusion Matrix of the model ')\n",
    "ax.xaxis.set_ticklabels(['Negative', 'Neutral' ,'Positive'])\n",
    "ax.yaxis.set_ticklabels(['Negative', 'Neutral' ,'Positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597366e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_pred = [np.round(model.predict([X_ids_test, X_mask_test])) for model in L_best_models]\n",
    "prediction = np.median(L_pred, axis=0)\n",
    "\n",
    "predicted_labels = np.argmax(prediction, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c31ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(Y_test, predicted_labels,target_names=['Negative','Neutral','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(Y_test , predicted_labels)\n",
    "f1 = f1_score(Y_test , predicted_labels, average=None)\n",
    "m1 = float(metric1(Y_test, predicted_labels))\n",
    "m2 = float(metric2_2(Y_test, predicted_labels))\n",
    "\n",
    "print(\"metric1 Test mean :\", m1, \"\\n\")\n",
    "print(\"metric2 Test mean :\", m2, \"\\n\")\n",
    "print(\"Average F1 score on negative/neutral/positive: \", np.mean(f1,axis=0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = sns.heatmap(cm, annot=True, fmt='.2%' , cmap=\"Blues\")\n",
    "ax.set_xlabel('\\n Predicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "ax.set_title(f'Confusion Matrix of the model ')\n",
    "ax.xaxis.set_ticklabels(['Negative', 'Neutral' ,'Positive'])\n",
    "ax.yaxis.set_ticklabels(['Negative', 'Neutral' ,'Positive'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
