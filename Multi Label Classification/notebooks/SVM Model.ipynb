{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers text-hammer pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f228ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup , AutoTokenizer, TFAutoModel, TFRobertaModel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "import text_hammer as th\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "import random as rd\n",
    "import keras.backend as K\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971a17f",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc605618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(model, validation_dataloader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    return average_val_loss\n",
    "\n",
    "def metric2(y_true, y_pred_classes):\n",
    "    n = y_true.size(0)\n",
    "    # Calculate errors where prediction is off by 1 class\n",
    "    res = torch.abs(y_true - y_pred_classes)\n",
    "    count_error = torch.sum(res == 1, dtype=torch.float32)\n",
    "    metric = 1 - count_error / n\n",
    "    return metric.item()\n",
    "\n",
    "def metric2_tf(y_true, y_pred):\n",
    "    # Calculate the differences between true and predicted labels\n",
    "    differences = np.abs(y_true - y_pred)\n",
    "    # Count the number of predictions that are off by 1 across all samples and labels\n",
    "    off_by_one = np.sum(differences == 1)\n",
    "    # Calculate the metric as the proportion of predictions that are not off by one\n",
    "    metric = 1 - (off_by_one / y_true.size)\n",
    "    return metric\n",
    "\n",
    "\n",
    "def metric2_2(y_true, y_pred):\n",
    "    # Calculate the differences between true and predicted labels\n",
    "    differences = np.abs(y_true - y_pred)\n",
    "    # Count the number of predictions that are off by 1 for each sample\n",
    "    off_by_one_per_sample = np.sum(differences == 1, axis=1)\n",
    "    # Count the number of samples where at least one prediction is off by 1\n",
    "    off_by_one = np.sum(off_by_one_per_sample > 0)\n",
    "    # Calculate the metric as the proportion of samples that are not off by one\n",
    "    metric = 1 - (off_by_one / len(y_true))\n",
    "    return metric\n",
    "\n",
    "def metric1_tf(y_true, y_pred):\n",
    "    n = tf.cast(tf.shape(y_true)[0], tf.float32)\n",
    "    y_pred_r = tf.round(y_pred)\n",
    "    res = tf.reduce_all(tf.equal(y_true, y_pred_r), axis=1)\n",
    "    res = tf.cast(res, tf.float32)\n",
    "    return tf.reduce_sum(res) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    mention = r'@\\w+'\n",
    "    hash = r'#\\w+'\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', '')\n",
    "    x = re.sub(r'[^\\x00-\\x7F]+', ' ', x)\n",
    "    x = th.cont_exp(x)\n",
    "    x = th.remove_emails(x)\n",
    "    x = th.remove_urls(x)\n",
    "    x = re.sub(mention, ' ', x)\n",
    "    x = re.sub(hash, ' ', x)\n",
    "    x = th.remove_html_tags(x)\n",
    "    x = th.remove_rt(x)\n",
    "    x = th.remove_accented_chars(x)\n",
    "    x = th.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    x = re.sub(r'\\w*\\d+\\w*', ' ', x).strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e4b31",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_spss(\"/content/drive/MyDrive/VA_EN_TU_2012-2020_3000_tweets_relevant_V03_labeled_1200_cleaned.sav\")\n",
    "data = df[['text', 'Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'] = data['text'].apply(get_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abaea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Individual Label Distribution\n",
    "label_sums = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].sum(axis=0)\n",
    "labels = ['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']\n",
    "\n",
    "# label_counts = data[['label_1_positive', 'label_2_negative', 'label_3_neutral']].sum()\n",
    "# print(label_counts)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(labels, label_sums)\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Individual Label Distribution')\n",
    "plt.show()\n",
    "\n",
    "## Label Combinations Distribution\n",
    "df_train_labels = pd.DataFrame(data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']], columns=labels)\n",
    "combinations = df_train_labels.groupby(labels).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "\n",
    "combinations = combinations.sort_values(by='count', ascending=False) # by count to get most frequent combinations\n",
    "combinations['Label Combination'] = combinations[labels].astype(int).astype(str).agg(','.join, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "combinations.plot(x='Label Combination', y='count', kind='bar', legend=False)\n",
    "plt.title('Label Combinations Distribution')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70780198",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "texts = data['text'].tolist()\n",
    "labels = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].values.astype(int)\n",
    "\n",
    "# Convert labels to a multi-label format\n",
    "y = np.array(labels)\n",
    "\n",
    "# TF-IDF features with n-grams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVM model with class weights\n",
    "svm_model = OneVsRestClassifier(SVC(kernel='linear', class_weight='balanced'))\n",
    "\n",
    "# hyperparameter grid search\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm_model, param_grid, scoring='f1_micro', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "metric2_score = metric2_tf(y_test, y_pred)\n",
    "metric1_score = metric1_tf(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Metric 2: {metric2_score}\")\n",
    "print(f\"Metric 1: {metric1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41daa656",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names =['negative', 'neutral','positive']\n",
    "print(classification_report(y_test, y_pred , target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print(f\"Hamming loss : {hamming_loss(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff22727",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['cleaned_text'].tolist()\n",
    "labels = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].values.astype(int)\n",
    "\n",
    "# Convert labels to a multi-label format\n",
    "y = np.array(labels)\n",
    "\n",
    "# TF-IDF features with n-grams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_model = OneVsRestClassifier(SVC(kernel='linear', class_weight='balanced'))\n",
    "\n",
    "# hyperparameter grid search\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm_model, param_grid, scoring='f1_micro', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "metric2_score = metric2_tf(y_test, y_pred)\n",
    "metric1_score = metric1_tf(y_test, y_pred)\n",
    "# results\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Metric 2: {metric2_score}\")\n",
    "print(f\"Metric 1: {metric1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61972ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names =['negative', 'neutral','positive']\n",
    "print(classification_report(y_test, y_pred , target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ba13a",
   "metadata": {},
   "source": [
    "#### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Sample data\n",
    "texts = data['text'].tolist()\n",
    "labels = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].values.astype(int)\n",
    "\n",
    "# Convert labels to a multi-label format\n",
    "y = np.array(labels)\n",
    "# Create TF-IDF features with n-grams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a multi-label SVM model\n",
    "svm_model = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='micro')  # Micro-average is often used in multi-label classification\n",
    "metric2_score = metric2_tf(y_test, y_pred)\n",
    "metric1_score = metric1_tf(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Metric 2: {metric2_score}\")\n",
    "print(f\"Metric 1: {metric1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names =['negative', 'neutral','positive']\n",
    "print(classification_report(y_test, y_pred , target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0fe0f",
   "metadata": {},
   "source": [
    "#### Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62590c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Sample data\n",
    "texts = data['cleaned_text'].tolist()\n",
    "labels = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].values.astype(int)\n",
    "\n",
    "# Convert labels to a multi-label format\n",
    "y = np.array(labels)\n",
    "# Create TF-IDF features with n-grams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a multi-label SVM model\n",
    "svm_model = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='micro')  # Micro-average is often used in multi-label classification\n",
    "metric2_score = metric2_tf(y_test, y_pred)\n",
    "metric1_score = metric1_tf(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Metric 2: {metric2_score}\")\n",
    "print(f\"Metric 1: {metric1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68967a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names =['negative', 'neutral','positive']\n",
    "print(classification_report(y_test, y_pred , target_names=label_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
