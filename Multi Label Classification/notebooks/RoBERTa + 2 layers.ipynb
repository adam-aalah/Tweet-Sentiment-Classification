{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers text-hammer pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f228ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup , AutoTokenizer, TFAutoModel, TFRobertaModel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "import text_hammer as th\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "import random as rd\n",
    "import keras.backend as K\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971a17f",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc605618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------STEP 1 --------------------------------#\n",
    "\n",
    "# Metric1 construction to compute the % of success tweet by tweet\n",
    "\n",
    "def metric1(y_true, y_pred):\n",
    "    n = K.cast(K.shape(y_true)[0], 'float32')\n",
    "    y_pred_r = K.round(y_pred)\n",
    "    res = K.all(K.equal(y_true, y_pred_r), axis=1)\n",
    "    res = K.cast(res, 'float32')\n",
    "    res = K.sum(res) / n\n",
    "    return res\n",
    "\n",
    "#------------------------STEP 2 --------------------------------#\n",
    "\n",
    "# Metric2 construction to compute the % of success label by label\n",
    "# (equal to binary-accuracy)\n",
    "\n",
    "def metric2(y_true, y_pred):\n",
    "    n = K.cast(K.shape(y_true)[0], 'float32')\n",
    "    y_pred_r = K.round(y_pred)\n",
    "    res = abs(y_true - y_pred_r)\n",
    "    count_error = K.sum(K.cast(K.equal(res, 1), 'float32'),axis=0)\n",
    "    res = 1-count_error / n\n",
    "    res = K.mean(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_true , y_pred):\n",
    "    cm = multilabel_confusion_matrix(y_true , y_pred)\n",
    "    return cm/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2_per_labels(M): # Takes a matrix containing the absolute difference between true value and prediction\n",
    "    for i in range(M.shape[1]):\n",
    "        s = np.sum(M,axis = 0) # Summing per column\n",
    "        s = 1-s/M.shape[0] # % of good predictions per column\n",
    "        s = np.round(s,3)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    mention = r'@\\w+'\n",
    "    hash = r'#\\w+'\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', '')\n",
    "    x = re.sub(r'[^\\x00-\\x7F]+', ' ', x)\n",
    "    x = th.cont_exp(x)\n",
    "    x = th.remove_emails(x)\n",
    "    x = th.remove_urls(x)\n",
    "    x = re.sub(mention, ' ', x)\n",
    "    x = re.sub(hash, ' ', x)\n",
    "    x = th.remove_html_tags(x)\n",
    "    x = th.remove_rt(x)\n",
    "    x = th.remove_accented_chars(x)\n",
    "    x = th.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    x = re.sub(r'\\w*\\d+\\w*', ' ', x).strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e4b31",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_spss(\"/content/drive/MyDrive/VA_EN_TU_2012-2020_3000_tweets_relevant_V03_labeled_1200_cleaned.sav\")\n",
    "data = df[['text', 'Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'] = data['text'].apply(get_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abaea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Individual Label Distribution\n",
    "label_sums = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].sum(axis=0)\n",
    "labels = ['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']\n",
    "\n",
    "# label_counts = data[['label_1_positive', 'label_2_negative', 'label_3_neutral']].sum()\n",
    "# print(label_counts)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(labels, label_sums)\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Individual Label Distribution')\n",
    "plt.show()\n",
    "\n",
    "## Label Combinations Distribution\n",
    "df_train_labels = pd.DataFrame(data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']], columns=labels)\n",
    "combinations = df_train_labels.groupby(labels).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "\n",
    "combinations = combinations.sort_values(by='count', ascending=False) # by count to get most frequent combinations\n",
    "combinations['Label Combination'] = combinations[labels].astype(int).astype(str).agg(','.join, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "combinations.plot(x='Label Combination', y='count', kind='bar', legend=False)\n",
    "plt.title('Label Combinations Distribution')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70780198",
   "metadata": {},
   "source": [
    "### RoBERTa model + 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "nb_samples = len(data) # Number of tweets\n",
    "proportion_training = 0.80 # 80/20 ratio : 80% of training data for k-fold validation, 20% for test data\n",
    "pretrained_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\" # Roberta model for sentiment\n",
    "seq_len = 128 # tweet length\n",
    "batch_size = 80\n",
    "learning_rate = 0.001\n",
    "nb_epoch = 60\n",
    "nb_fold = 3\n",
    "earlyStopPatience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41daa656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, max_length=seq_len,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   #return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b39a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing all data with Roberta's tokenizer\n",
    "X_ids = np.zeros((len(data),seq_len))\n",
    "X_mask = np.zeros((len(data),seq_len))\n",
    "\n",
    "for i, sentence in enumerate(data['text']):\n",
    "    X_ids[i, :], X_mask[i, :] = tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ac6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = round(proportion_training * len(data))\n",
    "labels = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].values\n",
    "# Creating training data\n",
    "X_ids_train = X_ids[:sep,:]\n",
    "X_mask_train = X_mask[:sep,:]\n",
    "Y_train = labels[:sep,:]\n",
    "\n",
    "# Creating test data\n",
    "X_ids_test = X_ids[sep:,:]\n",
    "X_mask_test = X_mask[sep:,:]\n",
    "Y_test = labels[sep:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained model\n",
    "pretrained_model = TFAutoModel.from_pretrained(pretrained_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f99f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    input_text_ids = keras.Input(shape=(seq_len,), dtype='int32', name='input_text_ids')\n",
    "    text_mask = keras.Input(shape=(seq_len,), dtype='int32', name='attention_mask_text')\n",
    "\n",
    "    embeddings = pretrained_model(input_text_ids, attention_mask=text_mask)[1]\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(embeddings)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.35)(x)\n",
    "\n",
    "    predictions = layers.Dense(3, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[input_text_ids,text_mask], outputs=predictions)\n",
    "\n",
    "    model.layers[2].trainable = False # freeze pretrained model layer\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=[metric1,metric2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7202d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_the_model(model, X_tr, y_tr, X_val, y_val, num):\n",
    "    # The model is trained on data input X_tr, y_tr, having X_val, y_val as validation data\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_metric2\", patience=earlyStopPatience, mode=\"max\"),\n",
    "        ModelCheckpoint(filepath=\"MODEL/best_model\"+str(num)+\".hdf5\", monitor=\"val_metric2\", mode='max', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    model.fit(X_tr, y_tr, validation_data = (X_val,y_val), epochs=nb_epoch, verbose=1, callbacks=callbacks, batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = [] # To save models\n",
    "list_history = [] # To save model efficiency\n",
    "\n",
    "kfold = KFold(n_splits=nb_fold, shuffle=True, random_state=42)\n",
    "\n",
    "# Training with k-fold method\n",
    "k = 1\n",
    "for train_index, val_index in kfold.split(X_ids_train,Y_train):\n",
    "    print(f'\\nTraining model {k}...')\n",
    "    model = get_model() # Model construction\n",
    "    history = fit_the_model(model,\n",
    "                            [X_ids_train[train_index],\n",
    "                             X_mask_train[train_index]],\n",
    "                            Y_train[train_index],\n",
    "                            [X_ids_train[val_index],\n",
    "                             X_mask_train[val_index]],\n",
    "                            Y_train[val_index], k) # Model training\n",
    "    list_of_models.append(history)\n",
    "    list_history.append(history.history.history)\n",
    "    k += 1\n",
    "\n",
    "print(\"---Finished---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1112102",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_fold):\n",
    "    print(50*'-',f\"MODEL PERFORMANCE {i+1}\",50*'-')\n",
    "\n",
    "    average_loss = list_history[i]['loss']\n",
    "    average_val_loss = list_history[i]['val_loss']\n",
    "\n",
    "    average_metric_1 = list_history[i]['metric1']\n",
    "    average_val_metric_1 = list_history[i]['val_metric1']\n",
    "\n",
    "    average_metric_2 = list_history[i]['metric2']\n",
    "    average_val_metric_2 = list_history[i]['val_metric2']\n",
    "\n",
    "    plt.figure(figsize=(16,4))\n",
    "\n",
    "    # Displays model loss and efficiency evolutions for each model\n",
    "    txtTitres = [\"Training and validation loss\",\"Training and validation metric 1\",\"Training and validation metric 2\"]\n",
    "\n",
    "    txtYLabel = [\"Loss\",\"Metric 1 \",\"Metric 2\"]\n",
    "\n",
    "    L1 = [average_loss, average_metric_1 , average_metric_2]\n",
    "    L2 = [average_val_loss, average_val_metric_1 , average_val_metric_2]\n",
    "\n",
    "    for k in range(3):\n",
    "        if len(L1[0]) < nb_epoch: # to avoid dimension errors when plotting\n",
    "            nb_epoch_plot=len(L1[0])\n",
    "        else:\n",
    "            nb_epoch_plot=nb_epoch\n",
    "        Epochs = range(1, nb_epoch_plot+1)\n",
    "        plt.subplot(1,3,k+1)\n",
    "        plt.plot(Epochs, L1[k], color='b', marker='o', label=\"training\")\n",
    "        plt.plot(Epochs, L2[k], color='r', marker='o', label=\"validation\")\n",
    "        plt.grid()\n",
    "        plt.title(txtTitres[k])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(txtYLabel[k])\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model for each fold, our final model will be the \"average\" of all of them\n",
    "L_best_models = []\n",
    "for i in range(nb_fold):\n",
    "    path = \"MODEL/best_model\" + str(i+1) + \".hdf5\"\n",
    "    L_best_models.append(keras.models.load_model(path,\n",
    "                                                 custom_objects={\"TFRobertaModel\": pretrained_model,\n",
    "                                                                 \"metric1\": metric1,\n",
    "                                                                 \"metric2\":metric2},\n",
    "                                                 compile=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfe63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing prediction of final model on validation data, accessible via each fold\n",
    "L_predictions = []\n",
    "L_cm = []\n",
    "L_f1 = []\n",
    "L_m1, L_m2, L_m3 = [], [], []\n",
    "\n",
    "k = 0\n",
    "\n",
    "# To evaluate our model which prediction will be the median of all predictions, we'll do an\n",
    "# average of all scores obtained by each model on their own validation_data\n",
    "kfold = KFold(n_splits=nb_fold, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kfold.split(X_ids_train,Y_train):\n",
    "\n",
    "    prediction = np.round(L_best_models[k].predict([X_ids_train[val_index], X_mask_train[val_index]])) # prediction for this fold's validation data\n",
    "\n",
    "    cm = ConfusionMatrix(Y_train[val_index] , prediction)\n",
    "    f1 = f1_score(Y_train[val_index] , prediction, average=None)\n",
    "    m1 = metric1(Y_train[val_index], prediction)\n",
    "    m2 = metric2(Y_train[val_index], prediction)\n",
    "    diff = np.abs(prediction-Y_train[val_index])\n",
    "    m3 = metric2_per_labels(diff)\n",
    "\n",
    "    L_predictions.append(prediction)\n",
    "    L_cm.append(cm)\n",
    "    L_f1.append(f1)\n",
    "    L_m1.append(m1)\n",
    "    L_m2.append(m2)\n",
    "    L_m3.append(m3)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4388b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the model's performances on validation data\n",
    "print(40*'-',f\"SUMMARY OF MODEL PERFORMANCE ON VALIDATION DATA\",20*'-',\"\\n\")\n",
    "\n",
    "# Average best metrics\n",
    "m1 = np.mean(L_m1,axis=0)\n",
    "m2 = np.mean(L_m2,axis=0)\n",
    "m3 = np.mean(L_m3,axis=0)\n",
    "\n",
    "print(\"metric1 validation mean :\", m1, \"\\n\")\n",
    "print(\"metric2 validation mean :\", m2, \"\\n\")\n",
    "print(\"metric2 validation mean per labels :\", m3, \"\\n\")\n",
    "\n",
    "# Average F1 score\n",
    "print(\"Average F1 score on negative/neutral/positive: \", np.mean(L_f1,axis=0))\n",
    "\n",
    "# Average confusion matrices\n",
    "avg_cm = np.mean(L_cm,axis=0)\n",
    "\n",
    "L_titles = [ 'Confusion Matrix for Negative label', 'Confusion Matrix for Neutral label','Confusion Matrix for Positive label']\n",
    "L_labels = [ 'Not Negative', 'Negative', 'Not Neutral', 'Neutral','Not Positive', 'Positive']\n",
    "\n",
    "k = 0\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    ax = sns.heatmap(avg_cm[i], annot=True, fmt='.2%' , cmap=\"Blues\")\n",
    "    ax.set_xlabel('\\n Predicted Values')\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "    ax.set_title(L_titles[i])\n",
    "    ax.xaxis.set_ticklabels(L_labels[k:k+2])\n",
    "    ax.yaxis.set_ticklabels(L_labels[k:k+2])\n",
    "    k += 2\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ff554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model for each fold, our final model will be the \"average\" of all of them\n",
    "L_best_models = []\n",
    "for i in range(nb_fold):\n",
    "    path = \"MODEL/best_model\" + str(i+1) + \".hdf5\"\n",
    "    L_best_models.append(keras.models.load_model(path,\n",
    "                                                 custom_objects={\"TFRobertaModel\": pretrained_model,\n",
    "                                                                 \"metric1\": metric1,\n",
    "                                                                 \"metric2\":metric2},\n",
    "                                                 compile=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing prediction of final model on test data\n",
    "\n",
    "L_pred = [np.round(model.predict([X_ids_test, X_mask_test])) for model in L_best_models]\n",
    "prediction = np.median(L_pred, axis=0) # prediction of final model on test data, that's where nb_fold needs to be odd\n",
    "\n",
    "cm = ConfusionMatrix(Y_test , prediction)\n",
    "f1 = f1_score(Y_test , prediction, average=None)\n",
    "m1 = float(metric1(Y_test, prediction))\n",
    "m2 = float(metric2(Y_test, prediction))\n",
    "diff = np.abs(prediction-Y_test)\n",
    "m3 = metric2_per_labels(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b22ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the model's performances on testing data\n",
    "print(40*'-',f\"SUMMARY OF MODEL PERFORMANCE ON TEST DATA\",20*'-',\"\\n\")\n",
    "\n",
    "# metrics\n",
    "print(\"metric1 Test mean :\", m1, \"\\n\")\n",
    "print(\"metric2 Test mean :\", m2, \"\\n\")\n",
    "print(\"metric2 Test mean per labels :\", m3, \"\\n\")\n",
    "\n",
    "# F1 score\n",
    "print(\"F1 score on positive/negative/neutral: \", f1)\n",
    "\n",
    "# Confusion matrix\n",
    "L_titles = [ 'Confusion Matrix for Negative label', 'Confusion Matrix for Neutral label','Confusion Matrix for Positive label']\n",
    "L_labels = [ 'Not Negative', 'Negative', 'Not Neutral', 'Neutral','Not Positive', 'Positive']\n",
    "\n",
    "k = 0\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    ax = sns.heatmap(cm[i], annot=True, fmt='.2%' , cmap=\"Blues\")\n",
    "    ax.set_xlabel('\\n Predicted Values')\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "    ax.set_title(L_titles[i])\n",
    "    ax.xaxis.set_ticklabels(L_labels[k:k+2])\n",
    "    ax.yaxis.set_ticklabels(L_labels[k:k+2])\n",
    "    k += 2\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb92b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print(f\"Hamming loss : {hamming_loss(Y_test, prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Negative', 'Neutral','Positive']\n",
    "print(classification_report(Y_test, prediction, target_names=label_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
