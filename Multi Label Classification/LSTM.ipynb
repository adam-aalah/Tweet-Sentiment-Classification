{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers text-hammer pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f228ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup , AutoTokenizer, TFAutoModel, TFRobertaModel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "import text_hammer as th\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "import random as rd\n",
    "import keras.backend as K\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971a17f",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc605618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2_tf(y_true, y_pred):\n",
    "    differences = tf.abs(y_true - tf.round(y_pred))\n",
    "    off_by_one = tf.reduce_sum(tf.cast(differences == 1, tf.float32))\n",
    "    total_elements = tf.cast(tf.size(y_true), tf.float32)\n",
    "    metric = 1 - (off_by_one / total_elements)\n",
    "    return metric\n",
    "\n",
    "def metric1_tf(y_true, y_pred):\n",
    "    n = tf.cast(tf.shape(y_true)[0], tf.float32)\n",
    "    y_pred_r = tf.round(y_pred)\n",
    "    res = tf.reduce_all(tf.equal(y_true, y_pred_r), axis=1)\n",
    "    res = tf.cast(res, tf.float32)\n",
    "    return tf.reduce_sum(res) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    mention = r'@\\w+'\n",
    "    hash = r'#\\w+'\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', '')\n",
    "    x = re.sub(r'[^\\x00-\\x7F]+', ' ', x)\n",
    "    x = th.cont_exp(x)\n",
    "    x = th.remove_emails(x)\n",
    "    x = th.remove_urls(x)\n",
    "    x = re.sub(mention, ' ', x)\n",
    "    x = re.sub(hash, ' ', x)\n",
    "    x = th.remove_html_tags(x)\n",
    "    x = th.remove_rt(x)\n",
    "    x = th.remove_accented_chars(x)\n",
    "    x = th.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    x = re.sub(r'\\w*\\d+\\w*', ' ', x).strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e4b31",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_spss(\"/content/drive/MyDrive/VA_EN_TU_2012-2020_3000_tweets_relevant_V03_labeled_1200_cleaned.sav\")\n",
    "data = df[['text', 'Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'] = data['text'].apply(get_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49b325",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb3e18",
   "metadata": {},
   "source": [
    "#### raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46967c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data['text'].values\n",
    "labels = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].values\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 64\n",
    "max_length = 70\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Tokenize and Pad sequences\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(padded, labels, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[metric1_tf, metric2_tf])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, metric1_score,metric2_score = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Metric 1: {metric1_score*100:.2f}%\")\n",
    "print(f\"Validation Metric 2: {metric2_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test = y_test.astype(int)\n",
    "threshold = 0.5\n",
    "y_pred_thresholded = (y_pred > threshold).astype(int)\n",
    "label_names = ['Negative', 'Neutral','Positive']\n",
    "\n",
    "\n",
    "acc_test = accuracy_score(y_test,y_pred_thresholded)\n",
    "metr_1_score = metric1_tf(y_test, y_pred_thresholded)\n",
    "metr_2_score = metric2_tf(y_test, y_pred_thresholded)\n",
    "print(classification_report(y_test, y_pred_thresholded, target_names=label_names))\n",
    "print(f\"Test Metric 2: {metr_2_score*100:.2f}%\")\n",
    "print(f\"Test Metric 1: {metr_1_score*100:.2f}%\")\n",
    "print(f\"Test Accuracy score: {acc_test*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print(f\"Hamming loss : {hamming_loss(y_test, y_pred_thresholded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef64241",
   "metadata": {},
   "source": [
    "#### cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data['cleaned_text'].values\n",
    "labels = data[['Label_A2_negative', 'Label_A3_neutral','Label_A1_positive']].values\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 64\n",
    "max_length = 70\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Tokenize and Pad sequences\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(padded, labels, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', metric2_tf])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48213529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy,metric2_score = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Validation Metric 2: {metric2_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8da98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test = y_test.astype(int)\n",
    "threshold = 0.5\n",
    "y_pred_thresholded = (y_pred > threshold).astype(int)\n",
    "label_names = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "\n",
    "acc_test = accuracy_score(y_test,y_pred_thresholded)\n",
    "metr_2_score = metric2_tf(y_test, y_pred_thresholded)\n",
    "print(classification_report(y_test, y_pred_thresholded, target_names=label_names))\n",
    "print(f\"Test Metric 2: {metr_2_score*100:.2f}%\")\n",
    "print(f\"Test Accuracy score: {acc_test*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metr_1_score = metric1_tf(y_test, y_pred_thresholded)\n",
    "print(f\"Test Metric 1: {metr_1_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print(f\"Hamming loss : {hamming_loss(y_test, y_pred_thresholded)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
